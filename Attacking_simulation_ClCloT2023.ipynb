{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8b09ab-ad05-4c28-aad3-c8d56ef8f495",
   "metadata": {},
   "source": [
    "# Testing on CICIoT2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e37e42-49f2-406a-a9a2-78a045be9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "from joblib import load, dump\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af9da7-7a4b-4537-8cbf-e4e8700e8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = os.getcwd()\n",
    "DATASET_DIRECTORY = './CICIoT2023/'\n",
    "DATASET_DIRECTORY = os.path.join(BASE, DATASET_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4a062-1a40-449f-93cd-ddca4e8313fe",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473ab4e-aad7-443b-9955-d368853f9610",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b565c7-aed8-4836-9837-a2d51b987edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "df_sets.sort()\n",
    "training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "test_sets = df_sets[int(len(df_sets)*.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf81d26-ed4a-4857-a582-c1748119ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = [\n",
    "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "       'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "       'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "       'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "       'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "       'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "       'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "y_column = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5efcb-2250-4c92-b403-29a6afba74f1",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c112a1-8cf9-4517-9311-4763f0c40f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5668a3-f8ac-4e7a-9293-1f5e8298db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_set in tqdm(training_sets):\n",
    "    scaler.fit(pd.read_csv(DATASET_DIRECTORY + train_set)[X_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f39de1-feaa-43d1-8773-30c6b6139884",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Classification: 2 (1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10d0c3-5a1b-423d-ae1a-2c43f5d9eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_2classes = {}\n",
    "dict_2classes['DDoS-RSTFINFlood'] = 'Attack'\n",
    "dict_2classes['DDoS-PSHACK_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-SYN_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-UDP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-TCP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-ICMP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-SynonymousIP_Flood'] = 'Attack'\n",
    "dict_2classes['DDoS-ACK_Fragmentation'] = 'Attack'\n",
    "dict_2classes['DDoS-UDP_Fragmentation'] = 'Attack'\n",
    "dict_2classes['DDoS-ICMP_Fragmentation'] = 'Attack'\n",
    "dict_2classes['DDoS-SlowLoris'] = 'Attack'\n",
    "dict_2classes['DDoS-HTTP_Flood'] = 'Attack'\n",
    "\n",
    "dict_2classes['DoS-UDP_Flood'] = 'Attack'\n",
    "dict_2classes['DoS-SYN_Flood'] = 'Attack'\n",
    "dict_2classes['DoS-TCP_Flood'] = 'Attack'\n",
    "dict_2classes['DoS-HTTP_Flood'] = 'Attack'\n",
    "\n",
    "\n",
    "dict_2classes['Mirai-greeth_flood'] = 'Attack'\n",
    "dict_2classes['Mirai-greip_flood'] = 'Attack'\n",
    "dict_2classes['Mirai-udpplain'] = 'Attack'\n",
    "\n",
    "dict_2classes['Recon-PingSweep'] = 'Attack'\n",
    "dict_2classes['Recon-OSScan'] = 'Attack'\n",
    "dict_2classes['Recon-PortScan'] = 'Attack'\n",
    "dict_2classes['VulnerabilityScan'] = 'Attack'\n",
    "dict_2classes['Recon-HostDiscovery'] = 'Attack'\n",
    "\n",
    "dict_2classes['DNS_Spoofing'] = 'Attack'\n",
    "dict_2classes['MITM-ArpSpoofing'] = 'Attack'\n",
    "\n",
    "dict_2classes['BenignTraffic'] = 'Benign'\n",
    "\n",
    "dict_2classes['BrowserHijacking'] = 'Attack'\n",
    "dict_2classes['Backdoor_Malware'] = 'Attack'\n",
    "dict_2classes['XSS'] = 'Attack'\n",
    "dict_2classes['Uploading_Attack'] = 'Attack'\n",
    "dict_2classes['SqlInjection'] = 'Attack'\n",
    "dict_2classes['CommandInjection'] = 'Attack'\n",
    "\n",
    "dict_2classes['DictionaryBruteForce'] = 'Attack'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268af74b-0882-4e63-a34f-9b83081eb090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "ML_models = [\n",
    "        LogisticRegression(n_jobs=-1),\n",
    "]\n",
    "\n",
    "ML_neams = [\n",
    "        \"LogisticRegression\",\n",
    "]\n",
    "\n",
    "\n",
    "for train_set in tqdm(training_sets):\n",
    "    d = pd.read_csv(DATASET_DIRECTORY + train_set)\n",
    "    d[X_columns] = scaler.transform(d[X_columns])\n",
    "    new_y = [dict_2classes[k] for k in d[y_column]]\n",
    "    d[y_column] = new_y\n",
    "    d[y_column] = d[y_column].apply(lambda x: 1 if x=='Attack' else 0)\n",
    "    \n",
    "    for model in (ML_models):\n",
    "        model.fit(d[X_columns], d[y_column])\n",
    "    del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15e2fd-f3dc-4179-b5df-0a9573dddd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "preds = {i:[] for i in range(len(ML_models))}\n",
    "for test_set in tqdm(test_sets):\n",
    "    d_test = pd.read_csv(DATASET_DIRECTORY + test_set)\n",
    "    d_test[X_columns] = scaler.transform(d_test[X_columns])\n",
    "    new_y = [dict_2classes[k] for k in d_test[y_column]]\n",
    "    d_test[y_column] = new_y\n",
    "    d_test[y_column] = d_test[y_column].apply(lambda x: 1 if x=='Attack' else 0)\n",
    "    y_test += list(d_test[y_column].values)\n",
    "    \n",
    "    for i in range(len(ML_models)):\n",
    "        model = ML_models[i]\n",
    "        y_pred = list(model.predict(d_test[X_columns]))\n",
    "        preds[i] = preds[i] + y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42863125-0dc3-48db-9a84-240cf4054a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "for k,v in preds.items():\n",
    "    y_pred = v\n",
    "    print(f\"##### {ML_neams[k]} (2 classes) #####\")\n",
    "    print('accuracy_score: ', accuracy_score(y_pred, y_test))\n",
    "    print('recall_score: ', recall_score(y_pred, y_test, average='macro'))\n",
    "    print('precision_score: ', precision_score(y_pred, y_test, average='macro'))\n",
    "    print('f1_score: ', f1_score(y_pred, y_test, average='macro'))\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0487ba-bec9-41b7-b133-238c31ccb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Saving the model\n",
    "dump(model, os.path.join(BASE, \"LogisticRegression_CiCioT2023.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e500a-6300-4200-9044-757a05d6948d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Important Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c36e22-c95e-4fd6-a727-54feb90cbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_importance(model_file, n):\n",
    "    '''\n",
    "    Return top n features of a Forest model\n",
    "    :param model_file: name of the saved model\n",
    "    :param n: top n features\n",
    "    '''\n",
    "    \n",
    "    a = load(model_file)\n",
    "    a_importances = a.feature_importances_\n",
    "    a_features_df = pd.DataFrame({\n",
    "        'Feature': X_columns, \n",
    "        'Importance': a_importances\n",
    "    })\n",
    "    a_features_df = a_features_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    return a_features_df.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302d5d6-0e3b-4565-9439-3058daa40389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_api(target, features, threshold=0.5, type='hard'):\n",
    "    '''\n",
    "    This function opens a model previously saved and uses it to classify some samples.\n",
    "    you can pass the feature line or a matrix of features, each sample a line\n",
    "    \n",
    "    : target: model used to classify feature's samples. This code use scikit way of predict \n",
    "    : features: feature vector to classify in pd Dataset format\n",
    "    : threshold: to use models which use threshold\n",
    "    : type: soft -> output vector with class and other information; hard -> output with only class\n",
    "    '''\n",
    "    actual_directory = os.getcwd()\n",
    "    target_file = os.path.join(actual_directory, target)\n",
    "    if os.path.exists(target_file):\n",
    "        # loading specific model (it needs to get the model file name to be opened)\n",
    "        target_model = load(os.path.join(actual_directory, target))\n",
    "    else:\n",
    "        print('Model file not found!')\n",
    "        return -1\n",
    "    \n",
    "    prediction = target_model.predict(features)\n",
    "    output = []\n",
    "\n",
    "    for result in prediction:\n",
    "\n",
    "        if result <= threshold:\n",
    "            if type == 'soft':\n",
    "                output.append((result, 0)) #benign\n",
    "            elif type == 'hard':\n",
    "                output.append(0)\n",
    "            else:\n",
    "                output.append(-1) #error\n",
    "        else:\n",
    "            if type == 'soft':\n",
    "                output.append((result, 1)) #malicious\n",
    "            elif type == 'hard':\n",
    "                output.append(1)\n",
    "            else:\n",
    "                output.append(-1) #error\n",
    "\n",
    "    if type == 'soft':\n",
    "        return pd.Series(output, dtype=object)\n",
    "    elif type == 'hard':\n",
    "        return pd.Series(output, dtype=np.int64)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc9e78-bb52-42c9-84f6-ce44c7b315f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_values_importance(shap_values_1, shap_values_2, k=15): \n",
    "    # Calculate EBFA for each instance\n",
    "    ebfa_scores = []\n",
    "    loop = (len(shap_values_1)) if (len(shap_values_1) < len(shap_values_2)) else (len(shap_values_2))\n",
    "    for i in range(loop):\n",
    "        # Extract SHAP values for the instance\n",
    "        shap_vals_instance_1 = shap_values_1[i]\n",
    "        shap_vals_instance_2 = shap_values_2[i]\n",
    "\n",
    "        importance_1 = np.abs(shap_vals_instance_1)\n",
    "        importance_2 = np.abs(shap_vals_instance_2)\n",
    "\n",
    "        # Determine top k features and convert indices to tuples\n",
    "        top_feats_1 = set(tuple(np.argsort(-importance_1)[:k]))\n",
    "        top_feats_2 = set(tuple(np.argsort(-importance_2)[:k]))\n",
    "\n",
    "        # Calculate intersection and EBFA score\n",
    "        intersection = top_feats_1.intersection(top_feats_2)\n",
    "        ebfa_score = len(intersection) / k\n",
    "        ebfa_scores.append(ebfa_score)\n",
    "\n",
    "    # Average EBFA value\n",
    "    average_ebfa = np.mean(ebfa_scores)\n",
    "    print('Average EBFA:', average_ebfa)\n",
    "\n",
    "    return average_ebfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755dc5a4-90e4-45f4-8803-dc33f58d4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(data, num_samples, lambda_):\n",
    "    synthetic_data = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # choose random two numbers\n",
    "        idx1, idx2 = np.random.choice(len(data), 2, replace=False)\n",
    "        point1, point2 = data.iloc[idx1], data.iloc[idx2]\n",
    "\n",
    "        if (lambda_):\n",
    "            factor = lambda_\n",
    "        else:\n",
    "            factor = np.random.uniform(0,1)\n",
    "\n",
    "        new_point = factor * point1 + (1 - factor) * point2\n",
    "        synthetic_data.append(tuple(new_point))\n",
    "    new_data = np.array(list(set(synthetic_data))) # set for avoiding repetition\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3e0b5-2912-4701-9db8-d1d394b0ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_noise(data, noise_level=0.01):\n",
    "    noise = np.random.normal(0, noise_level * np.std(data, axis=0), data.shape)\n",
    "    new_data = data + noise\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d09a46-a094-426c-87ca-5c9a37433c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating clusters to use as background data in our attack ''' \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def process_and_scale_data(X, y, n_clusters, n_init=10):\n",
    "    ''' Generate normalized centroids'''\n",
    "\n",
    "    df = X.copy()\n",
    "    df['label'] = y\n",
    "    \n",
    "    # Dividindo o DataFrame com base nas labels\n",
    "    df_0 = df[df['label'] == 0].drop('label', axis=1)\n",
    "    df_1 = df[df['label'] == 1].drop('label', axis=1)\n",
    "\n",
    "    # Treinando modelos KMeans\n",
    "    kmeans_0 = KMeans(n_clusters=n_clusters, n_init=n_init, random_state=25).fit(df_0)\n",
    "    kmeans_1 = KMeans(n_clusters=n_clusters, n_init=n_init, random_state=25).fit(df_1)\n",
    "\n",
    "    # Extraindo centróides\n",
    "    centroids_0 = kmeans_0.cluster_centers_\n",
    "    centroids_1 = kmeans_1.cluster_centers_\n",
    "\n",
    "    # Criando DataFrames para os centróides\n",
    "    centroids_df_0 = pd.DataFrame(centroids_0, columns=df_0.columns)\n",
    "    centroids_df_1 = pd.DataFrame(centroids_1, columns=df_1.columns)\n",
    "\n",
    "    # Normalizando os dados\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_df_0 = pd.DataFrame(scaler.fit_transform(centroids_df_0), columns=centroids_df_0.columns)\n",
    "    scaled_df_1 = pd.DataFrame(scaler.fit_transform(centroids_df_1), columns=centroids_df_1.columns)\n",
    "\n",
    "    return scaled_df_0, scaled_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da49500-e2c1-4a16-970a-449286f793cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_model_our_attack(target, X, y, augment_count=2, string='aa', lambda_=0.7, ebfa_limit=0.6, mode='li', surrogate_algo='RandomForestClassifier', threshold=0.5):\n",
    "    '''\n",
    "\n",
    "    :Param X: Attack data\n",
    "    :Param y: Attack label-data\n",
    "    :Param lambda_:\n",
    "    :Param augment_count:\n",
    "    :Param detection_variation:\n",
    "    '''\n",
    "\n",
    "    # config\n",
    "    max_iter = 20 # max iterations to avoid an eternal loop\n",
    "    num_samples = 200 #stating number of each class sample. eg: 5 benign and 5 malicious samples\n",
    "    file_name = string+'ebfa_limit_'+str(ebfa_limit)+'_surrogate_model_our_attack_'+surrogate_algo+'_'+target+'_'+mode \n",
    "    substitute_model_path = os.path.join(os.getcwd(), file_name)\n",
    "    target_model = load(target)\n",
    "    if surrogate_algo == 'RandomForestClassifier':\n",
    "        surrogate_model = RandomForestClassifier(random_state=38)\n",
    "    elif surrogate_algo == 'DecisionTreeClassifier':\n",
    "        surrogate_model = DecisionTreeClassifier(criterion='log_loss')\n",
    "    elif surrogate_algo == 'KNNClassifier':\n",
    "        surrogate_model = KNeighborsClassifier(n_neighbors = 5)\n",
    "    elif surrogate_algo == 'QDAClassifier':\n",
    "        surrogate_model = QuadraticDiscriminantAnalysis()\n",
    "    elif surrogate_algo == 'ADABoostClassifier':\n",
    "        surrogate_model = AdaBoostClassifier(random_state=38)\n",
    "    elif surrogate_algo == 'LogisticRegression':\n",
    "        surrogate_model = LogisticRegression(random_state=38)\n",
    "    elif surrogate_algo == 'MLPClassifier':\n",
    "        surrogate_model = MLPClassifier(hidden_layer_sizes=(154, 154), activation='relu', solver='sgd', alpha=1e-4, learning_rate_init=0.001, max_iter=100, random_state=38, verbose=True)\n",
    "    else:\n",
    "        print(f\"Option {surrogate_algo} isn't within algorithms options.\")\n",
    "        return -1\n",
    "            \n",
    "    \n",
    "    # reset unpaired index and drop unlabeled rows\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    label_index = (y != -1)\n",
    "    X_train = X[label_index]\n",
    "    y_train = y[label_index]\n",
    "\n",
    "    # creating subsamples (index)\n",
    "    index_0 = (y <= threshold)\n",
    "    index_1 = (y > threshold)\n",
    "    \n",
    "\n",
    "    # test samples to be used in all of the validations, including to calculate the prediction value of the target model\n",
    "    test_index_0 = np.random.choice(np.where(index_0)[0], 500, replace=False)\n",
    "    test_index_1 = np.random.choice(np.where(index_1)[0], 500, replace=False)\n",
    "    test_sample = np.concatenate((test_index_0, test_index_1))\n",
    "    np.random.shuffle(test_sample)\n",
    "    X_test = X_train.iloc[test_sample]\n",
    "    y_test = y_train.iloc[test_sample]\n",
    "\n",
    "    # Removing test data from original dataset, avoiding data leak\n",
    "    X_train = X_train.drop(test_sample, axis=0).reset_index(drop=True)\n",
    "    y_train = y_train.drop(test_sample, axis=0).reset_index(drop=True)\n",
    "\n",
    "    # re-calculating the index without the test samples\n",
    "    index_0 = (y_train <= threshold)   # index_0 - index_0(test)\n",
    "    index_1 = (y_train > threshold)    # index_1 - index_1(test)\n",
    "\n",
    "    # Shap config\n",
    "    X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=0.01, stratify=y_train)\n",
    "    _, X_test_sample, _, y_test_sample = train_test_split(X_test, y_test, test_size=0.1, stratify=y_test)\n",
    "    X_test_sample = pd.DataFrame(X_test_sample, columns=X.columns)\n",
    "    background_data = X_test_sample.map(lambda x: 0)\n",
    "\n",
    "    \n",
    "    # calculating shap-sampling to the target model\n",
    "    print('Calculating Shap values of the target model')\n",
    "    explainer_1 = shap.SamplingExplainer(target_model.predict, pd.concat([process_and_scale_data(X_train, y_train, n_clusters=10, n_init=10)], columns=X_train.columns))\n",
    "    shap_values_1 = explainer_1.shap_values(X_test_sample) \n",
    "\n",
    "    for iter_count in tqdm.tqdm(range(max_iter), desc='Substitute model training'):\n",
    "        print(f'Iteration {iter_count + 1}: Number of samples = {num_samples * 2}')\n",
    "        balanced_sample_0 = np.random.choice(np.where(index_0)[0], num_samples, replace=False)\n",
    "        balanced_sample_1 = np.random.choice(np.where(index_1)[0], num_samples, replace=False)\n",
    "        balanced_sample = np.concatenate((balanced_sample_0, balanced_sample_1))\n",
    "        np.random.shuffle(balanced_sample)\n",
    "\n",
    "        #creating REAL-DATA subsampling\n",
    "        X_subsample_real = X_train.iloc[balanced_sample]\n",
    "        y_subsample_real = classify_api(target, X_subsample_real, threshold=0.5, type='hard')\n",
    "        \n",
    "        #creating synthetic-data subsampling\n",
    "        if mode == 'li': #linear interpolation\n",
    "            synthetic_data_0 = linear_interpolation(X_train.iloc[balanced_sample_0], (int(round(num_samples * augment_count))), lambda_)\n",
    "            synthetic_data_1 = linear_interpolation(X_train.iloc[balanced_sample_1], (int(round(num_samples * augment_count))), lambda_)\n",
    "            print(f'Creating {int(len(synthetic_data_0)) + int(len(synthetic_data_1))} synthetic samples using Linear Interpolation')\n",
    "\n",
    "        elif mode == 'gn': # gaussian noise\n",
    "            synthetic_data_0 = gaussian_noise(X_train.iloc[balanced_sample_0], noise_level=lambda_)\n",
    "            synthetic_data_1 = gaussian_noise(X_train.iloc[balanced_sample_1], noise_level=lambda_)\n",
    "            print(f'Creating {int(len(synthetic_data_0)) + int(len(synthetic_data_1))} syntetic samples using Gaussian noise')\n",
    "\n",
    "        else:\n",
    "            print(\"Incorrect option.\")\n",
    "            break\n",
    "        \n",
    "        # Concatenating real samples with synthetic samples\n",
    "        X_subsample = np.concatenate((X_subsample_real, synthetic_data_0, synthetic_data_1))\n",
    "        X_subsample_df = pd.DataFrame(X_subsample, columns=X.columns) # changing to pd dataframe\n",
    "\n",
    "        synthetic_y_0 = np.array([0] * len(synthetic_data_0))\n",
    "        synthetic_y_1 = np.array([0] * len(synthetic_data_1))\n",
    "        y_subsample = np.concatenate((y_subsample_real, synthetic_y_0, synthetic_y_1))\n",
    "        y_subsample_df = pd.Series(y_subsample)\n",
    "\n",
    "        \n",
    "        print(f'Total of {len(X_subsample_df)} samples')\n",
    "        surrogate_model.fit(X_subsample_df, y_subsample_df)\n",
    "        dump(surrogate_model, \"surrogate_model_intermediate_our_attack.joblib\")\n",
    "        print('Calculating EBFA...')\n",
    "        explainer_2 = shap.SamplingExplainer(target_model.predict,  pd.concat([process_and_scale_data(X_train_sample, y_train_sample, n_clusters=10, n_init=10)], columns=X_train.columns))\n",
    "        shap_values_2 = explainer_2.shap_values(X_test_sample)\n",
    "        ebfa_models = shap_values_importance(shap_values_1, shap_values_2, k=15)\n",
    "        \n",
    "        print(\"=======================================================================================================\")\n",
    "        print(f'Queries: {(num_samples*2):.4f}')\n",
    "        print(f'Explainability-Based Feature Agreement: {ebfa_models:.4f}')\n",
    "        print(\"=======================================================================================================\")\n",
    "        \n",
    "        temp_file = os.path.join(os.getcwd(), \"surrogate_model_intermediate_our_attack.joblib\")\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "\n",
    "        if abs(ebfa_models) >= ebfa_limit:\n",
    "            print(\"Saving substitute model\")\n",
    "            dump(surrogate_model, substitute_model_path)\n",
    "            \n",
    "            print(f'Convergence achieved after {iter_count +1}. Saving model at: {substitute_model_path}')\n",
    "            return (num_samples * 2, ebfa_models)\n",
    "        # if continuing, the number of samples is enlarged\n",
    "        num_samples += 200\n",
    "        if num_samples > len(index_0) or num_samples > len(index_1):\n",
    "            print(f\"Process finished after {iter_count +1}. The convergence wasn't achieved.\")\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2907f9-0ca0-489d-b849-5665b00a34a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Attacking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319e381-5c26-42c3-844d-4a84b36fb25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando normalização diferente daquela usada nos dados de treinamento original\n",
    "Ascaler = MinMaxScaler()\n",
    "X_scaled = Ascaler.fit_transform(d_test[X_columns])\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100fae8-22f7-406b-a4f0-44f7693aa461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "substitute_model_our_attack(\"LogisticRegression_CiCioT2023.joblib\", X_scaled, d_test[y_column], lambda_=0.02, ebfa_limit=0.5, mode='li', surrogate_algo='RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8321f87-0089-4281-88d7-1f55ff9b7117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "substitute_model_our_attack(\"LogisticRegression_CiCioT2023.joblib\", X_scaled, d_test[y_column], lambda_=0.02, ebfa_limit=0.8, mode='li', surrogate_algo='RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dbf167-7500-4699-8a00-c8e2a095cd4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "substitute_model_our_attack(\"LogisticRegression_CiCioT2023.joblib\", X_scaled, d_test[y_column], lambda_=0.6, ebfa_limit=0.8, mode='gn', surrogate_algo='DecisionTreeClassifier')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3860312-9e2b-4518-afef-e88d0c157141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "substitute_model_our_attack(\"LogisticRegression_CiCioT2023.joblib\", X_scaled, d_test[y_column], lambda_=0.6, ebfa_limit=0.8, mode='li', surrogate_algo='DecisionTreeClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54834bc5-c8c8-427b-9503-33fb488bb986",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training a Second model to compare the features - RandomForestClassifier as Surrogate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa250c3e-dfc3-412a-ab37-fbfe62937cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_models = [\n",
    "        RandomForestClassifier(random_state=45),\n",
    "]\n",
    "\n",
    "ML_neams = [\n",
    "        \"RandomForestClassifier\",\n",
    "]\n",
    "\n",
    "\n",
    "for train_set in tqdm(training_sets):\n",
    "    d = pd.read_csv(DATASET_DIRECTORY + train_set)\n",
    "    d[X_columns] = scaler.transform(d[X_columns])\n",
    "    new_y = [dict_2classes[k] for k in d[y_column]]\n",
    "    d[y_column] = new_y\n",
    "    d[y_column] = d[y_column].apply(lambda x: 1 if x=='Attack' else 0)\n",
    "    \n",
    "    for model in (ML_models):\n",
    "        model.fit(d[X_columns], d[y_column])\n",
    "    del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10df8ab-70bd-4dcb-b05f-160d67fca667",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "preds = {i:[] for i in range(len(ML_models))}\n",
    "for test_set in tqdm(test_sets):\n",
    "    d_test = pd.read_csv(DATASET_DIRECTORY + test_set)\n",
    "    d_test[X_columns] = scaler.transform(d_test[X_columns])\n",
    "    new_y = [dict_2classes[k] for k in d_test[y_column]]\n",
    "    d_test[y_column] = new_y\n",
    "    d_test[y_column] = d_test[y_column].apply(lambda x: 1 if x=='Attack' else 0)\n",
    "    y_test += list(d_test[y_column].values)\n",
    "    \n",
    "    for i in range(len(ML_models)):\n",
    "        model = ML_models[i]\n",
    "        y_pred = list(model.predict(d_test[X_columns]))\n",
    "        preds[i] = preds[i] + y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0145a-7d51-4fc7-87ad-e0588a6c30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "for k,v in preds.items():\n",
    "    y_pred = v\n",
    "    print(f\"##### {ML_neams[k]} (2 classes) #####\")\n",
    "    print('accuracy_score: ', accuracy_score(y_pred, y_test))\n",
    "    print('recall_score: ', recall_score(y_pred, y_test, average='macro'))\n",
    "    print('precision_score: ', precision_score(y_pred, y_test, average='macro'))\n",
    "    print('f1_score: ', f1_score(y_pred, y_test, average='macro'))\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790bd90-5a3a-48ce-9e0c-4aa4e3c0f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Saving the model\n",
    "dump(model, os.path.join(BASE, \"RandomForestClassify_CiCioT2023.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d63082-cf6b-40eb-8953-cab5c862a2c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "substitute_model_our_attack(\"RandomForestClassify_CiCioT2023.joblib\", X_scaled, d_test[y_column], lambda_=0.02, ebfa_limit=0.4, mode='li', surrogate_algo='RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19406fc9-9a5e-4ae9-84f6-4bb43ccd3443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "substitute_model_our_attack(\"RandomForestClassify_CiCioT2023.joblib\", X_scaled, d_test[y_column], lambda_=0.2, ebfa_limit=0.6, mode='li', surrogate_algo='DecisionTreeClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9e029-79bb-419a-95b3-fa295aacccd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "substitute_model_our_attack(\"RandomForestClassify_CiCioT2023.joblib\", X_scaled, d_test[y_column], lambda_=0.2, ebfa_limit=0.7, mode='li', surrogate_algo='DecisionTreeClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476579b-81ae-4568-9c36-4d6e2cce17ee",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "###### substitute_model_our_attack_v2(\"RandomForestClassify_CiCioT2023.joblib\", X_scaled, d_test[y_column], lambda_=0.02, ebfa_limit=0.6, mode='li', surrogate_algo='RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671cd8a-dd5f-441c-8941-43f9c946367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_model_our_attack(\"RandomForestClassify_CiCioT2023.joblib\", X_scaled, d_test[y_column], lambda_=0.5, ebfa_limit=0.5, mode='li', surrogate_algo='RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8662f9-8d73-4fbc-be73-dda29b6748e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance(\"RandomForestClassify_CiCioT2023.joblib\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fb36c-94e1-43e9-a127-db615ba8f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance(\"aaebfa_limit_0.5_surrogate_model_our_attack_RandomForestClassifier_RandomForestClassify_CiCioT2023.joblib_li\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00ee70-144d-43c5-9ad7-35231171c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance(\"RandomForestClassify_CiCioT2023.joblib\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3635c-7117-49c4-94c8-84bcaf057d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance(\"aaebfa_limit_0.6_surrogate_model_our_attack_RandomForestClassifier_RandomForestClassify_CiCioT2023.joblib_li\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67c0b0-ecb9-44bb-925f-747b293e3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same scaler of the trained model\n",
    "Sscaler = StandardScaler()\n",
    "X_scaled_S = Sscaler.fit_transform(d_test[X_columns])\n",
    "X_scaled_S = pd.DataFrame(X_scaled_S, columns=[X_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d94272-c2b4-484b-96cf-3ff3719dcb55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "substitute_model_our_attack(\"RandomForestClassify_CiCioT2023.joblib\", X_scaled_S, d_test[y_column], lambda_=0.02, ebfa_limit=0.5, mode='gn', surrogate_algo='RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c430c-3cbf-4c64-8285-23c54ac030ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "substitute_model_our_attack(\"RandomForestClassify_CiCioT2023.joblib\", X_scaled_S, d_test[y_column], lambda_=0.02, ebfa_limit=0.5, mode='li', surrogate_algo='RandomForestClassifier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
